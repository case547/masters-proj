{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid2x2 using RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import ray\n",
    "from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from envs import MultiAgentSumoEnv\n",
    "from observation import Grid2x2ObservationFunction\n",
    "from reward_functions import combined_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "TEST_NUM = 2\n",
    "SEED = 23423  # default SUMO seed no.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "ENV_NAME = \"grid2x2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_env_creator(args):\n",
    "    env_params = {\n",
    "        \"net_file\": os.path.join(\"nets\",ENV_NAME,f\"{ENV_NAME}.net.xml\"),\n",
    "        \"route_file\": os.path.join(\"nets\",ENV_NAME,f\"{ENV_NAME}.rou.xml\"),\n",
    "        \"num_seconds\": 3600,\n",
    "        \"reward_fn\": combined_reward,\n",
    "        \"sumo_seed\": SEED,\n",
    "        \"observation_class\": Grid2x2ObservationFunction,\n",
    "        \"add_system_info\": False,\n",
    "    }\n",
    "    congestion_reward = combined_reward.__defaults__[0].__name__\n",
    "    alpha = combined_reward.__defaults__[1]  # congestion component coefficient\n",
    "    print(congestion_reward, alpha)\n",
    "\n",
    "    env = MultiAgentSumoEnv(**env_params)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_wait_time_reward 0.875\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# From https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/atari-ppo.yaml\n",
    "\n",
    "train_env = MultiAgentEnvCompatibility(train_env_creator({}))\n",
    "\n",
    "config: PPOConfig\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=ENV_NAME)\n",
    "    .framework(framework=\"torch\")\n",
    "    .rollouts(\n",
    "        rollout_fragment_length=100,\n",
    "        num_rollout_workers=10,\n",
    "    )\n",
    "    .training(\n",
    "        lambda_=0.95,\n",
    "        kl_coeff=0.5,\n",
    "        clip_param=0.1,\n",
    "        vf_clip_param=10.0,\n",
    "        entropy_coeff=0.01,\n",
    "        train_batch_size=1000,\n",
    "        sgd_minibatch_size=100,\n",
    "        num_sgd_iter=10,\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_duration=1,\n",
    "        evaluation_num_workers=1,\n",
    "        evaluation_sample_timeout_s=300,\n",
    "    )\n",
    "    .debugging(seed=SEED)\n",
    "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"1\")))\n",
    "    .multi_agent(\n",
    "        policies=set(train_env.env.ts_ids),\n",
    "        policy_mapping_fn=(lambda agent_id, *args, **kwargs: agent_id),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.join(\"outputs\",ENV_NAME,f\"test_{TEST_NUM}\")\n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Untrained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_creator(csv_path, tb_log_dir):\n",
    "    env_params = {\n",
    "        \"net_file\": os.path.join(\"nets\",ENV_NAME,f\"{ENV_NAME}.net.xml\"),\n",
    "        \"route_file\": os.path.join(\"nets\",ENV_NAME,f\"{ENV_NAME}.rou.xml\"),\n",
    "        \"num_seconds\": 3600,\n",
    "        \"reward_fn\": combined_reward,\n",
    "        \"sumo_seed\": SEED,\n",
    "        \"observation_class\": Grid2x2ObservationFunction,\n",
    "        \"add_system_info\": False,\n",
    "    }\n",
    "    congestion_reward = combined_reward.__defaults__[0].__name__\n",
    "    alpha = combined_reward.__defaults__[1]  # congestion component coefficient\n",
    "    print(congestion_reward, alpha)\n",
    "\n",
    "    env = MultiAgentSumoEnv(eval=True, csv_path=csv_path, tb_log_dir=tb_log_dir, **env_params)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 18:01:27,925\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "\n",
    "csv_path = os.path.join(csv_dir, \"untrained.csv\")\n",
    "tb_log_dir = os.path.join(\"logs\", ENV_NAME, f\"PPO_{TEST_NUM}\", \"eval_untrained\")\n",
    "\n",
    "with open(csv_path, \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow([\"sim_time\", \"arrived_num\", \"sys_tyre_pm\", \"sys_stopped\",\n",
    "                         \"sys_total_wait\", \"sys_avg_wait\", \"sys_avg_speed\",\n",
    "                         \"agents_tyre_pm\", \"agents_stopped\", \"agents_total_wait\",\n",
    "                         \"agents_avg_speed\", \"agents_total_pressure\",])\n",
    "\n",
    "register_env(ENV_NAME, lambda config: MultiAgentEnvCompatibility(env_creator(csv_path, tb_log_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 18:01:31,583\tWARNING algorithm_config.py:784 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "2023-06-01 18:01:31,613\tINFO algorithm.py:527 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=24656)\u001b[0m delta_wait_time_reward 0.875\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24656)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 9ms, vehicles TOT 0 ACT 0 BUF 0)                      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=22568)\u001b[0m 2023-06-01 18:01:38,670\tWARNING env.py:285 -- Your MultiAgentEnv <MultiAgentEnvCompatibility instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "2023-06-01 18:01:42,455\tWARNING algorithm_config.py:784 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=21976)\u001b[0m delta_wait_time_reward 0.875\u001b[32m [repeated 10x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19200)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 27ms, vehicles TOT 0 ACT 0 BUF 0)                     \u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=21976)\u001b[0m 2023-06-01 18:01:46,712\tWARNING env.py:285 -- Your MultiAgentEnv <MultiAgentEnvCompatibility instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "2023-06-01 18:01:46,812\tINFO trainable.py:172 -- Trainable.setup took 15.202 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3600.00 (1ms ~= 1000.00*RT, ~231000.00UPS, TraCI: 263ms, vehicles TOT 1715 ACT 231 B32ms, vehicles TOT 3 ACT 3 BUF 0)      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~231000.00UPS, TraCI: 263ms, vehicles TOT 1715 ACT 231 B32ms, vehicles TOT 3 ACT 3 BUF 0)      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': -320852.8744288877,\n",
       "  'episode_reward_min': -320852.8744288877,\n",
       "  'episode_reward_mean': -320852.8744288877,\n",
       "  'episode_len_mean': 720.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 1,\n",
       "  'policy_reward_min': {'1': -76832.12950596321,\n",
       "   '2': -83696.77696430053,\n",
       "   '5': -79422.03550824955,\n",
       "   '6': -80901.93245037406},\n",
       "  'policy_reward_max': {'1': -76832.12950596321,\n",
       "   '2': -83696.77696430053,\n",
       "   '5': -79422.03550824955,\n",
       "   '6': -80901.93245037406},\n",
       "  'policy_reward_mean': {'1': -76832.12950596321,\n",
       "   '2': -83696.77696430053,\n",
       "   '5': -79422.03550824955,\n",
       "   '6': -80901.93245037406},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [-320852.8744288877],\n",
       "   'episode_lengths': [720],\n",
       "   'policy_1_reward': [-76832.12950596321],\n",
       "   'policy_2_reward': [-83696.77696430053],\n",
       "   'policy_5_reward': [-79422.03550824955],\n",
       "   'policy_6_reward': [-80901.93245037406]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 1.467065903746966,\n",
       "   'mean_inference_ms': 2.762784574299679,\n",
       "   'mean_action_processing_ms': 0.22714544764504188,\n",
       "   'mean_env_wait_ms': 349.5278312164603,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0,\n",
       "   'StateBufferConnector_ms': 0.0,\n",
       "   'ViewRequirementAgentConnector_ms': 0.0731050968170166},\n",
       "  'num_agent_steps_sampled_this_iter': 2880,\n",
       "  'num_env_steps_sampled_this_iter': 720,\n",
       "  'timesteps_this_iter': 720}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=25400)\u001b[0m delta_wait_time_reward 0.875\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25400)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 15ms, vehicles TOT 0 ACT 0 BUF 0)                     \u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 18:25:42,196\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "\n",
    "register_env(ENV_NAME, lambda config: MultiAgentEnvCompatibility(train_env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 18:25:45,745\tWARNING algorithm_config.py:784 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=12764)\u001b[0m delta_wait_time_reward 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14920)\u001b[0m 2023-06-01 18:25:52,511\tWARNING env.py:285 -- Your MultiAgentEnv <MultiAgentEnvCompatibility instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14920)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 8ms, vehicles TOT 0 ACT 0 BUF 0)                      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 18:25:52,849\tWARNING algorithm_config.py:784 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "2023-06-01 18:25:57,062\tINFO trainable.py:172 -- Trainable.setup took 11.311 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=8648)\u001b[0m delta_wait_time_reward 0.875\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1400):  # 720 * 1400 == 1_008_000 total timesteps\n",
    "    results = algo.train()\n",
    "\n",
    "    if (i+1) % 200 == 0:\n",
    "        algo.save(os.path.join(\"ray_checkpoints\",\"grid2x2\",f\"test_{TEST_NUM}\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_dir = os.path.join(\"outputs\",ENV_NAME,f\"test_{TEST_NUM}\")\n",
    "# if not os.path.exists(csv_dir):\n",
    "#     os.makedirs(csv_dir)\n",
    "\n",
    "# csv_path = os.path.join(csv_dir, \"untrained.csv\")\n",
    "# tb_log_dir = os.path.join(\"logs\", ENV_NAME, \"eval_untrained\")\n",
    "\n",
    "# with open(csv_path, \"a\", newline=\"\") as f:\n",
    "#     csv_writer = csv.writer(f)\n",
    "#     csv_writer.writerow([\"sim_time\", \"arrived_num\", \"sys_tyre_pm\", \"sys_stopped\",\n",
    "#                          \"sys_total_wait\", \"sys_avg_wait\", \"sys_avg_speed\",\n",
    "#                          \"agents_tyre_pm\", \"agents_stopped\", \"agents_total_wait\",\n",
    "#                          \"agents_avg_speed\", \"agents_total_pressure\",])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
