{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid4x4 - PettingZoo + RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ray\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23758ce12d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 23423  # default SUMO seed no.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supersuit as ss\n",
    "from reward_functions import combined_reward\n",
    "\n",
    "env_name = \"grid2x2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs import MultiAgentSumoEnv\n",
    "from observation import Grid2x2ObservationFunction\n",
    "\n",
    "def env_creator(args):\n",
    "    env_params = {\n",
    "        \"net_file\": os.path.join(\"nets\",env_name,f\"{env_name}.net.xml\"),\n",
    "        \"route_file\": os.path.join(\"nets\",env_name,f\"{env_name}.rou.xml\"),\n",
    "        \"num_seconds\": 3600,\n",
    "        \"reward_fn\": combined_reward,\n",
    "        \"sumo_seed\": SEED,\n",
    "        \"observation_class\": Grid2x2ObservationFunction,\n",
    "        \"add_system_info\": False,\n",
    "    }\n",
    "    env = MultiAgentSumoEnv(**env_params)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 23:25:58,555\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "\n",
    "ray.init()\n",
    "\n",
    "register_env(env_name, lambda config: MultiAgentEnvCompatibility(env_creator(config)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# From https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/atari-ppo.yaml\n",
    "\n",
    "config: PPOConfig\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=env_name)\n",
    "    .framework(framework=\"torch\")\n",
    "    .rollouts(\n",
    "        num_rollout_workers=4, rollout_fragment_length=128,\n",
    "        # num_envs_per_worker=5,\n",
    "        # batch_mode=\"truncate_episodes\",\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size=512,\n",
    "        lr=2e-5,\n",
    "        gamma=0.99,\n",
    "        lambda_=0.9,\n",
    "        use_gae=True,\n",
    "        clip_param=0.4,\n",
    "        grad_clip=None,\n",
    "        entropy_coeff=0.1,\n",
    "        vf_loss_coeff=0.25,\n",
    "        sgd_minibatch_size=64,\n",
    "        num_sgd_iter=10,\n",
    "        # lambda_=0.95,\n",
    "        # kl_coeff=0.5,\n",
    "        # clip_param=0.1,\n",
    "        # vf_clip_param=10.0,\n",
    "        # entropy_coeff=0.01,\n",
    "        # train_batch_size=5000,\n",
    "        # sgd_minibatch_size=500,\n",
    "        # num_sgd_iter=10,\n",
    "    )\n",
    "    .evaluation(\n",
    "        # evaluation_duration=3600,\n",
    "        # evaluation_duration_unit=\"timesteps\",\n",
    "        evaluation_num_workers=1,\n",
    "    )\n",
    "    .debugging(log_level=\"WARN\", seed=SEED)\n",
    "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"1\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 23:26:02,905\tWARNING algorithm_config.py:784 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "2023-05-31 23:26:02,938\tINFO algorithm.py:527 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=13784)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 8ms, vehicles TOT 0 ACT 0 BUF 0)                      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=24016)\u001b[0m 2023-05-31 23:26:07,701\tWARNING env.py:285 -- Your MultiAgentEnv <MultiAgentEnvCompatibility instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "2023-05-31 23:26:09,509\tWARNING algorithm_config.py:784 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29436)\u001b[0m 2023-05-31 23:26:13,935\tWARNING env.py:285 -- Your MultiAgentEnv <MultiAgentEnvCompatibility instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "2023-05-31 23:26:13,983\tINFO trainable.py:172 -- Trainable.setup took 11.048 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=29436)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 12ms, vehicles TOT 0 ACT 0 BUF 0)                     \u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3600.00 (1ms ~= 1000.00*RT, ~182000.00UPS, TraCI: 199ms, vehicles TOT 2022 ACT 182 B34ms, vehicles TOT 3 ACT 3 BUF 0)      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~228000.00UPS, TraCI: 226ms, vehicles TOT 1913 ACT 228 BOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (2ms ~= 500.00*RT, ~122500.00UPS, TraCI: 214ms, vehicles TOT 1458 ACT 245 BU32ms, vehicles TOT 3 ACT 3 BUF 0)      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~162000.00UPS, TraCI: 212ms, vehicles TOT 1994 ACT 162 BOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (2ms ~= 500.00*RT, ~111500.00UPS, TraCI: 214ms, vehicles TOT 1585 ACT 223 BUOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (2ms ~= 500.00*RT, ~115000.00UPS, TraCI: 214ms, vehicles TOT 1451 ACT 230 BUOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~220000.00UPS, TraCI: 231ms, vehicles TOT 1717 ACT 220 BOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~229000.00UPS, TraCI: 208ms, vehicles TOT 1676 ACT 229 BOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~232000.00UPS, TraCI: 232ms, vehicles TOT 1788 ACT 232 BOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~239000.00UPS, TraCI: 216ms, vehicles TOT 1614 ACT 239 BOT 3 ACT 3 BUF 0)                      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': -172951.6673387349,\n",
       "  'episode_reward_min': -455934.2899789813,\n",
       "  'episode_reward_mean': -313990.761392424,\n",
       "  'episode_len_mean': 720.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 10,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [-172951.6673387349,\n",
       "    -218625.77240152424,\n",
       "    -455934.2899789813,\n",
       "    -176077.63037164957,\n",
       "    -385718.7365816415,\n",
       "    -442348.2225033426,\n",
       "    -323648.8439603477,\n",
       "    -337031.8295013124,\n",
       "    -286961.70627487445,\n",
       "    -340608.9150118316],\n",
       "   'episode_lengths': [720, 720, 720, 720, 720, 720, 720, 720, 720, 720]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 1.5209045298909696,\n",
       "   'mean_inference_ms': 1.1370092642802263,\n",
       "   'mean_action_processing_ms': 0.2551095682024177,\n",
       "   'mean_env_wait_ms': 117.73392356546235,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0,\n",
       "   'StateBufferConnector_ms': 0.0,\n",
       "   'ViewRequirementAgentConnector_ms': 0.46590328216552734},\n",
       "  'num_agent_steps_sampled_this_iter': 28800,\n",
       "  'num_env_steps_sampled_this_iter': 7200,\n",
       "  'timesteps_this_iter': 7200}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 done!\n",
      "Iteration 2 done!\n",
      "Iteration 3 done!\n",
      "Iteration 4 done!\n",
      "Iteration 5 done!\n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~157000.00UPS, TraCI: 113ms, vehicles TOT 2094 ACT 157 B35ms, vehicles TOT 3 ACT 3 BUF 0)      \n",
      "Step #3600.00 (2ms ~= 500.00*RT, ~127000.00UPS, TraCI: 134ms, vehicles TOT 1521 ACT 254 BU35ms, vehicles TOT 3 ACT 3 BUF 0)      \n",
      "Step #3600.00 (2ms ~= 500.00*RT, ~127500.00UPS, TraCI: 133ms, vehicles TOT 1903 ACT 255 BUOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~271000.00UPS, TraCI: 103ms, vehicles TOT 1679 ACT 271 BOT 3 ACT 3 BUF 0)                      \n",
      "Iteration 6 done!\n",
      "Iteration 7 done!\n",
      "Iteration 8 done!\n",
      "Iteration 9 done!\n",
      "Iteration 10 done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for i in range(10):\n",
    "    algo.train()\n",
    "    print(f\"Iteration {i+1} done!\")\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        algo.save(os.path.join(\"ray_checkpoints\",\"grid2x2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #3600.00 (1ms ~= 1000.00*RT, ~125000.00UPS, TraCI: 168ms, vehicles TOT 2106 ACT 125 BOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~186000.00UPS, TraCI: 177ms, vehicles TOT 1500 ACT 186 BOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~88000.00UPS, TraCI: 153ms, vehicles TOT 2106 ACT 88 BUFOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~230000.00UPS, TraCI: 177ms, vehicles TOT 1899 ACT 230 BOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (0ms ?*RT. ?UPS, TraCI: 160ms, vehicles TOT 2106 ACT 112 BUF 0)             OT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (0ms ?*RT. ?UPS, TraCI: 154ms, vehicles TOT 2099 ACT 121 BUF 7)             OT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~110000.00UPS, TraCI: 158ms, vehicles TOT 2106 ACT 110 BOT 3 ACT 3 BUF 0)                      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~215000.00UPS, TraCI: 175ms, vehicles TOT 2027 ACT 215 B24ms, vehicles TOT 3 ACT 3 BUF 0)      \n",
      "Step #3600.00 (1ms ~= 1000.00*RT, ~119000.00UPS, TraCI: 152ms, vehicles TOT 2099 ACT 119 B26ms, vehicles TOT 3 ACT 3 BUF 0)      \n",
      "Step #3600.00 (0ms ?*RT. ?UPS, TraCI: 164ms, vehicles TOT 2106 ACT 125 BUF 0)             24ms, vehicles TOT 3 ACT 3 BUF 0)      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': -130306.71221711389,\n",
       "  'episode_reward_min': -388866.99752421613,\n",
       "  'episode_reward_mean': -182555.8182178615,\n",
       "  'episode_len_mean': 720.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 10,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [-145568.20744274557,\n",
       "    -388866.99752421613,\n",
       "    -148103.49036157245,\n",
       "    -248713.81197247276,\n",
       "    -138754.53009340414,\n",
       "    -130306.71221711389,\n",
       "    -139718.62651089474,\n",
       "    -194547.91902648786,\n",
       "    -146844.5645265106,\n",
       "    -144133.32250319677],\n",
       "   'episode_lengths': [720, 720, 720, 720, 720, 720, 720, 720, 720, 720]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 1.4675512850909556,\n",
       "   'mean_inference_ms': 1.0730825392010064,\n",
       "   'mean_action_processing_ms': 0.24676591139288054,\n",
       "   'mean_env_wait_ms': 93.95156260306916,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': 0.0,\n",
       "   'StateBufferConnector_ms': 0.009167194366455078,\n",
       "   'ViewRequirementAgentConnector_ms': 0.21555423736572266},\n",
       "  'num_agent_steps_sampled_this_iter': 28800,\n",
       "  'num_env_steps_sampled_this_iter': 7200,\n",
       "  'timesteps_this_iter': 7200}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
