{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor, is_vecenv_wrapped\n",
    "from sumo_rl import parallel_env\n",
    "import supersuit as ss\n",
    "\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "from sumo_rl import SumoEnvironment\n",
    "\n",
    "from reward_functions import diff_wait_time, tyre_pm\n",
    "    \n",
    "env_params = {\n",
    "    \"net_file\": os.path.join(\"nets\",\"simple_nets\",\"cross1ltl\",\"net.net.xml\"),\n",
    "    \"route_file\": os.path.join(\"nets\",\"simple_nets\",\"cross1ltl\",\"input_routes.rou.xml\"),\n",
    "    \"num_seconds\": 1200,\n",
    "    \"single_agent\": True,\n",
    "    \"reward_fn\": diff_wait_time,\n",
    "    \"sumo_seed\": 42,\n",
    "}\n",
    "env = SumoEnvironment(**env_params)\n",
    "check_env(env)\n",
    "env = Monitor(env)  # wrap env to know episode reward, length, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import linear_schedule\n",
    "\n",
    "# Using hyperparams for Atari (except for n_steps) from\n",
    "# https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=1024,\n",
    "    batch_size=256,\n",
    "    n_epochs=4,\n",
    "    clip_range=0.1,\n",
    "    ent_coef=1e-3,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DummyVecEnv' object has no attribute 'traffic_signals'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluate(model, env, n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Justin Mak\\masters-proj\\evaluate.py:112\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, env, csv_path, tb_log_dir, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[0;32m    106\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(\n\u001b[0;32m    107\u001b[0m         observations,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    108\u001b[0m         state\u001b[39m=\u001b[39mstates,\n\u001b[0;32m    109\u001b[0m         episode_start\u001b[39m=\u001b[39mepisode_starts,\n\u001b[0;32m    110\u001b[0m         deterministic\u001b[39m=\u001b[39mdeterministic,\n\u001b[0;32m    111\u001b[0m     )\n\u001b[1;32m--> 112\u001b[0m     new_observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m    113\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n\u001b[0;32m    114\u001b[0m     current_lengths \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Justin Mak\\anaconda3\\envs\\rl\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:171\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\Justin Mak\\anaconda3\\envs\\rl\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:57\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     55\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 57\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     58\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     59\u001b[0m         )\n\u001b[0;32m     60\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     61\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\Justin Mak\\anaconda3\\envs\\rl\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32mc:\\Users\\Justin Mak\\anaconda3\\envs\\rl\\lib\\site-packages\\sumo_rl\\environment\\env.py:311\u001b[0m, in \u001b[0;36mSumoEnvironment.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_actions(action)\n\u001b[1;32m--> 311\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_steps()\n\u001b[0;32m    313\u001b[0m observations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_observations()\n\u001b[0;32m    314\u001b[0m rewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_rewards()\n",
      "File \u001b[1;32mc:\\Users\\Justin Mak\\anaconda3\\envs\\rl\\lib\\site-packages\\sumo_rl\\environment\\env.py:328\u001b[0m, in \u001b[0;36mSumoEnvironment._run_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    326\u001b[0m time_to_act \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m time_to_act:\n\u001b[1;32m--> 328\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sumo_step()\n\u001b[0;32m    329\u001b[0m     \u001b[39mfor\u001b[39;00m ts \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mts_ids:\n\u001b[0;32m    330\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraffic_signals[ts]\u001b[39m.\u001b[39mupdate()\n",
      "File \u001b[1;32mc:\\Users\\Justin Mak\\anaconda3\\envs\\rl\\lib\\site-packages\\sumo_rl\\environment\\env.py:400\u001b[0m, in \u001b[0;36mSumoEnvironment._sumo_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sumo_step\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 400\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msumo\u001b[39m.\u001b[39;49msimulationStep()\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:335\u001b[0m, in \u001b[0;36mConnection.simulationStep\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m    333\u001b[0m     responses\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readSubscription(result))\n\u001b[0;32m    334\u001b[0m     numSubs \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 335\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmanageStepListeners(step)\n\u001b[0;32m    336\u001b[0m \u001b[39mreturn\u001b[39;00m responses\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\step.py:42\u001b[0m, in \u001b[0;36mStepManager.manageStepListeners\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m     40\u001b[0m listenersToRemove \u001b[39m=\u001b[39m []\n\u001b[0;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m listenerID, listener \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stepListeners\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> 42\u001b[0m     keep \u001b[39m=\u001b[39m listener\u001b[39m.\u001b[39;49mstep(step)\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m keep:\n\u001b[0;32m     44\u001b[0m         listenersToRemove\u001b[39m.\u001b[39mappend(listenerID)\n",
      "File \u001b[1;32mc:\\Users\\Justin Mak\\masters-proj\\sim_listener.py:48\u001b[0m, in \u001b[0;36mSimListener.step\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39m# In this time step\u001b[39;00m\n\u001b[0;32m     46\u001b[0m stats \u001b[39m=\u001b[39m defaultdict(\u001b[39mfloat\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49munwrapped\u001b[39m.\u001b[39;49mtraffic_signals)\n\u001b[0;32m     50\u001b[0m \u001b[39m# Sum stats over all traffic light agents\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, VecEnv):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DummyVecEnv' object has no attribute 'traffic_signals'"
     ]
    }
   ],
   "source": [
    "evaluate(model, env, n_eval_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "if not isinstance(env, VecEnv):\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "\n",
    "is_monitor_wrapped = is_vecenv_wrapped(env, VecMonitor) or env.env_is_wrapped(Monitor)[0]\n",
    "is_monitor_wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval_episodes = 1\n",
    "\n",
    "n_envs = env.num_envs\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "n_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_counts = np.zeros(n_envs, dtype=\"int\")\n",
    "# Divides episodes among different sub environments in the vector as evenly as possible\n",
    "episode_count_targets = np.array([(n_eval_episodes + i) // n_envs for i in range(n_envs)], dtype=\"int\")\n",
    "\n",
    "print(episode_counts)\n",
    "print(episode_count_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_rewards = np.zeros(n_envs)\n",
    "current_lengths = np.zeros(n_envs, dtype=\"int\")\n",
    "observations = env.reset()\n",
    "states = None\n",
    "episode_starts = np.ones((env.num_envs,), dtype=bool)\n",
    "\n",
    "print(observations.shape)\n",
    "print(episode_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions, states = model.predict(\n",
    "    observations,  # type: ignore[arg-type]\n",
    "    state=states,\n",
    "    episode_start=episode_starts,\n",
    "    deterministic=True,\n",
    ")\n",
    "print(actions)\n",
    "print(len(actions))\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observations, rewards, dones, infos = env.step(actions)\n",
    "\n",
    "print(rewards)\n",
    "print(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_lengths += 1\n",
    "current_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_envs):\n",
    "    if episode_counts[i] < episode_count_targets[i]:\n",
    "        # unpack values so that the callback can access the local variables\n",
    "        reward = rewards[i]\n",
    "        done = dones[i]\n",
    "        info = infos[i]\n",
    "        episode_starts[i] = done\n",
    "\n",
    "print(dones)\n",
    "print(episode_starts)\n",
    "\n",
    "observations = new_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traci\n",
    "num_steps = (env_params[\"num_seconds\"]-5 - traci.simulation.getTime())/5\n",
    "int(num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(int(num_steps)):\n",
    "    actions, states = model.predict(\n",
    "        observations,  # type: ignore[arg-type]\n",
    "        state=states,\n",
    "        episode_start=episode_starts,\n",
    "        deterministic=True,\n",
    "    )\n",
    "    new_observations, rewards, dones, infos = env.step(actions)\n",
    "    current_rewards += rewards\n",
    "    current_lengths += 1\n",
    "\n",
    "    observations = new_observations\n",
    "\n",
    "print(dones)\n",
    "print(current_rewards)\n",
    "print(current_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traci.simulation.getTime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions, states = model.predict(\n",
    "    observations,  # type: ignore[arg-type]\n",
    "    state=states,\n",
    "    episode_start=episode_starts,\n",
    "    deterministic=True,\n",
    ")\n",
    "new_observations, rewards, dones, infos = env.step(actions)\n",
    "current_rewards += rewards\n",
    "current_lengths += 1\n",
    "\n",
    "print(dones)\n",
    "print(infos)\n",
    "print(current_rewards)\n",
    "print(current_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = new_observations\n",
    "traci.simulation.getTime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
