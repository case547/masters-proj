from collections import Counter
import csv
import os
import time
from typing import List, Optional, Union

import numpy as np
from torch.utils.tensorboard import SummaryWriter

from gymnasium.spaces import Discrete
from gymnasium.utils import EzPickle
from pettingzoo.utils import agent_selector

from sumo_rl import SumoEnvironment
from sumo_rl.environment.env import SumoEnvironmentPZ
import traci

from helper_functions import get_total_waiting_time, get_tyre_pm


LIBSUMO = "LIBSUMO_AS_TRACI" in os.environ


class CountAllRewardsEnv(SumoEnvironment):
    """Environment that counts rewards every sumo_step.
    
    Because delta_time != 1, the reward given to the agent(s) every
    step() is the sum of the last delta_time rewards generated by SUMO.
    """
    def __init__(self, **kwargs):
        # Call the parent constructor
        super().__init__(**kwargs)
    
    def step(self, action: Union[dict, int]):
        """Apply the action(s) and then step the simulation for delta_time seconds.

        Args:
            action (Union[dict, int]): action(s) to be applied to the environment.
            If single_agent is True, action is an int, otherwise it expects a dict with keys corresponding to traffic signal ids.
        """
        # No action, follow fixed TL defined in self.phases
        if action is None or action == {}:
            # Rewards for the sumo steps between every env step
            self.reward_hold = Counter({ts: 0 for ts in self.ts_ids})

            for _ in range(self.delta_time):
                self._sumo_step()

                r = {ts: self.traffic_signals[ts].compute_reward() for ts in self.ts_ids}
                self.reward_hold.update(r)  # add r to reward_hold Counter

        else:
            self._apply_actions(action)
            self._run_steps()

        observations = self._compute_observations()
        rewards = self._compute_rewards()
        dones = self._compute_dones()
        terminated = False  # there are no 'terminal' states in this environment
        truncated = dones["__all__"]  # episode ends when sim_step >= max_steps
        info = self._compute_info()

        if self.single_agent:
            return observations[self.ts_ids[0]], rewards[self.ts_ids[0]], terminated, truncated, info
        else:
            return observations, rewards, dones, info
    
    def _run_steps(self):
        # Rewards for the sumo steps between every env step
        self.reward_hold = Counter({ts: 0 for ts in self.ts_ids})
        time_to_act = False
        while not time_to_act:
            self._sumo_step()
            r = {ts: self.traffic_signals[ts].compute_reward() for ts in self.ts_ids}
            self.reward_hold.update(r)  # add r to reward_hold Counter

            for ts in self.ts_ids:
                self.traffic_signals[ts].update()
                if self.traffic_signals[ts].time_to_act:
                    time_to_act = True

    def _compute_rewards(self):
        self.rewards.update(
            {ts: self.reward_hold[ts] for ts in self.ts_ids if self.traffic_signals[ts].time_to_act}
        )
        return {ts: self.rewards[ts] for ts in self.rewards.keys() if self.traffic_signals[ts].time_to_act}


class CountAllRewardsEnvPZ(SumoEnvironmentPZ):
    """A wrapper for `CountAllRewardsEnv` that implements the AECEnv interface from PettingZoo."""
    def __init__(self, **kwargs):
        EzPickle.__init__(self, **kwargs)
        self._kwargs = kwargs

        self.seed()
        self.env = CountAllRewardsEnv(**self._kwargs)  # instead of SumoEnvironment

        self.agents = self.env.ts_ids
        self.possible_agents = self.env.ts_ids
        self._agent_selector = agent_selector(self.agents)
        self.agent_selection = self._agent_selector.reset()
        # spaces
        self.action_spaces = {a: self.env.action_spaces(a) for a in self.agents}
        self.observation_spaces = {a: self.env.observation_spaces(a) for a in self.agents}

        # dicts
        self.rewards = {a: 0 for a in self.agents}
        self.terminations = {a: False for a in self.agents}
        self.truncations = {a: False for a in self.agents}
        self.infos = {a: {} for a in self.agents}


class MultiAgentSumoEnv(CountAllRewardsEnv):
    def __init__(self, eval=False, csv_path: Optional[str] = None, tb_log_dir: Optional[str] = None, **kwargs):
        super().__init__(**kwargs)
        self.eval = eval
        self.csv_path = csv_path
        self.tb_log_dir = tb_log_dir

        if tb_log_dir:
            self.tb_writer = SummaryWriter(tb_log_dir)  # prep TensorBoard

        # Initialise cumulative counters
        self.tyre_pm_system = 0
        self.tyre_pm_agents = 0
        self.arrived_so_far = 0

        # Get traffic lights and lanes they control
        self.controlled_lanes = []
        for ts in self.traffic_signals.values():
            self.controlled_lanes += ts.lanes

        self.max_dist = 200

        if csv_path:
            with open(csv_path, "a", newline="") as f:
                csv_writer = csv.writer(f)
                headers = (["sim_time", "arrived_num", "sys_tyre_pm", "sys_stopped",
                            "sys_total_wait", "sys_avg_wait", "sys_avg_speed",
                            "agents_tyre_pm", "agents_stopped", "agents_total_delay", "agents_total_wait",
                            "agents_avg_delay", "agents_avg_wait", "agents_avg_speed"])
                csv_writer.writerow(headers)

    def _apply_actions(self, actions):
        """Set the next green phase for the traffic signals.

        Actions outside the original action space refer to action value 0.

        Args:
            actions: dict {ts_id : greenPhase}
        """
        for ts, action in actions.items():
            if self.traffic_signals[ts].time_to_act:
                n = self.action_spaces(ts).n
                if action > n - 1:
                    action = 0
                self.traffic_signals[ts].set_next_phase(action)

    def _compute_info(self):
        info = {"__common__": {"step": self.sim_step}}
        per_agent_info = self._get_per_agent_info()

        for agent_id in self.ts_ids:
            agent_info = {}

            for k, v in per_agent_info.items():
                if k.startswith(agent_id):
                    agent_info[k.split("_")[-1]] = v

            # Add tyre PM
            agent_info["tyre_pm"] = get_tyre_pm(self.traffic_signals[agent_id])

            info.update({agent_id: agent_info})

        return info
    
    @property
    def action_space(self):
        """Return the biggest action space of the traffic signal agents."""
        spaces = [self.action_spaces(ts) for ts in self.traffic_signals]
        max_n = max([space.n for space in spaces])
        return Discrete(max_n)

    def _sumo_step(self):
        self.sumo.simulationStep()

        if self.eval:
            # SYSTEM
            vehicles = self.sumo.vehicle.getIDList()
            speeds = [self.sumo.vehicle.getSpeed(veh) for veh in vehicles]
            waiting_times = [self.sumo.vehicle.getWaitingTime(veh) for veh in vehicles]

            system_tyre_pm = get_tyre_pm()
            arrived_num = self.sumo.simulation.getArrivedNumber()
            self.tyre_pm_system += system_tyre_pm
            self.arrived_so_far += arrived_num
            
            system_stats = {
                "total_stopped": sum(int(speed < 0.1) for speed in speeds),
                "total_waiting_time": sum(waiting_times),
                "mean_waiting_time": 0.0 if len(vehicles) == 0 else np.mean(waiting_times),
                "mean_speed": 0.0 if len(vehicles) == 0 else np.mean(speeds),
            }
            
            # AGENTS
            observable_vehs = []
            for lane in self.controlled_lanes:
                observable_vehs += self.get_observable_vehs(lane)

            agent_speeds = [self.sumo.vehicle.getSpeed(veh) for veh in observable_vehs]
            agent_waiting_times = [self.sumo.vehicle.getWaitingTime(veh) for veh in observable_vehs]
            agent_accum_waits = [self.sumo.vehicle.getAccumulatedWaitingTime(veh) for veh in observable_vehs]
            
            agents_tyre_pm = sum(abs(self.sumo.vehicle.getAcceleration(veh)) for veh in observable_vehs)
            self.tyre_pm_agents += agents_tyre_pm
            
            agent_stats = {
                # ts: TrafficSignal
                "total_stopped": sum(int(speed < 0.1) for speed in agent_speeds),
                "total_accum_wait": sum(agent_accum_waits),
                "total_waiting_time": sum(agent_waiting_times),
                
                "avg_accum_wait": 0.0 if len(observable_vehs) == 0 else np.mean(agent_accum_waits),
                "avg_waiting_time": 0.0 if len(observable_vehs) == 0 else np.mean(agent_waiting_times),
                "avg_speed": 0.0 if len(observable_vehs) == 0 else np.mean(agent_speeds),
            }
            
            # Log to CSV
            if self.csv_path:
                with open(self.csv_path, "a", newline="") as f:
                    csv_writer = csv.writer(f)
                    data = ([self.sim_step, arrived_num, system_tyre_pm]
                            + list(system_stats.values())
                            + [agents_tyre_pm]
                            + list(agent_stats.values()))
                    
                    csv_writer.writerow(data)

            # Log to TensorBoard
            if hasattr(self, "tb_writer"):
                # System
                self.tb_writer.add_scalar("world/arrived_so_far", self.arrived_so_far, self.sim_step)
                self.tb_writer.add_scalar("world/tyre_pm_cumulative", self.tyre_pm_system, self.sim_step)

                for stat, val in system_stats.items():
                    self.tb_writer.add_scalar(f"world/{stat}", val, self.sim_step)

                # Agents
                self.tb_writer.add_scalar("agents/tyre_pm_cumulative", self.tyre_pm_agents, self.sim_step)

                for stat, val in agent_stats.items():
                    self.tb_writer.add_scalar(f"agents/{stat}", val, self.sim_step)

    def close(self):
        """Close the environment and stop the SUMO simulation."""
        if self.sumo is None:
            return

        if not LIBSUMO:
            traci.switch(self.label)
        traci.close()

        # Help completely release SUMO port between episodes to address
        # "Unable to create listening socket: Address already in use" error
        time.sleep(2)

        if self.disp is not None:
            self.disp.stop()
            self.disp = None

        self.sumo = None

    def get_observable_vehs(self, lane) -> List[str]:
        """Remove undetectable vehicles from a lane."""
        detectable = []
        for vehicle in self.sumo.lane.getLastStepVehicleIDs(lane):
            path = self.sumo.vehicle.getNextTLS(vehicle)
            if len(path) > 0:
                next_light = path[0]
                distance = next_light[2]
                if distance <= self.max_dist:  # Detectors have a max range
                    detectable.append(vehicle)
        return detectable