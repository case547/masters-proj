{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid2x2 using RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import ray\n",
    "from ray.rllib.env.wrappers.multi_agent_env_compatibility import MultiAgentEnvCompatibility\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from envs import MultiAgentSumoEnv\n",
    "from observation import Cologne8ObservationFunction\n",
    "from reward_functions import combined_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "TEST_NUM = 1\n",
    "SEED = 23423  # default SUMO seed no.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "ENV_NAME = \"cologne8\"\n",
    "OBS_CLASS = Cologne8ObservationFunction\n",
    "assert not os.path.exists(os.path.join(\"ray_checkpoints\",ENV_NAME,f\"test_{TEST_NUM}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_env_creator(args):\n",
    "    env_params = {\n",
    "        \"net_file\": os.path.join(\"nets\",ENV_NAME,f\"{ENV_NAME}.net.xml\"),\n",
    "        \"route_file\": os.path.join(\"nets\",ENV_NAME,f\"{ENV_NAME}.rou.xml\"),\n",
    "        \"num_seconds\": 3600,\n",
    "        \"reward_fn\": combined_reward,\n",
    "        \"sumo_seed\": SEED,\n",
    "        \"observation_class\": OBS_CLASS,\n",
    "        \"add_system_info\": False,\n",
    "    }\n",
    "    congestion_reward = combined_reward.__defaults__[0].__name__\n",
    "    alpha = combined_reward.__defaults__[1]  # congestion component coefficient\n",
    "    print(congestion_reward, alpha)\n",
    "\n",
    "    env = MultiAgentSumoEnv(**env_params)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# From https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/atari-ppo.yaml\n",
    "\n",
    "train_env = MultiAgentEnvCompatibility(train_env_creator({}))\n",
    "\n",
    "config: PPOConfig\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=ENV_NAME)\n",
    "    .framework(framework=\"torch\")\n",
    "    .rollouts(\n",
    "        rollout_fragment_length=100,\n",
    "        num_rollout_workers=10,\n",
    "    )\n",
    "    .training(\n",
    "        lambda_=0.95,\n",
    "        kl_coeff=0.5,\n",
    "        clip_param=0.1,\n",
    "        vf_clip_param=10.0,\n",
    "        entropy_coeff=0.01,\n",
    "        train_batch_size=1000,\n",
    "        sgd_minibatch_size=100,\n",
    "        num_sgd_iter=10,\n",
    "    )\n",
    "    .evaluation(\n",
    "        evaluation_duration=1,\n",
    "        evaluation_num_workers=1,\n",
    "        evaluation_sample_timeout_s=300,\n",
    "    )\n",
    "    .debugging(seed=SEED)\n",
    "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"1\")))\n",
    "    .multi_agent(\n",
    "        policies=set(train_env.env.ts_ids),\n",
    "        policy_mapping_fn=(lambda agent_id, *args, **kwargs: agent_id),\n",
    "    )\n",
    "    .fault_tolerance(recreate_failed_workers=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.join(\"outputs\",ENV_NAME,f\"test_{TEST_NUM}\")\n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Untrained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_env_creator(csv_path: Optional[str] = None, tb_log_dir: Optional[str] = None):\n",
    "    env_params = {\n",
    "        \"net_file\": os.path.join(\"nets\",ENV_NAME,f\"{ENV_NAME}.net.xml\"),\n",
    "        \"route_file\": os.path.join(\"nets\",ENV_NAME,f\"{ENV_NAME}.rou.xml\"),\n",
    "        \"num_seconds\": 3600,\n",
    "        \"reward_fn\": combined_reward,\n",
    "        \"sumo_seed\": SEED,\n",
    "        \"observation_class\": OBS_CLASS,\n",
    "        \"add_system_info\": False,\n",
    "    }\n",
    "    congestion_reward = combined_reward.__defaults__[0].__name__\n",
    "    alpha = combined_reward.__defaults__[1]  # congestion component coefficient\n",
    "    print(congestion_reward, alpha)\n",
    "\n",
    "    env = MultiAgentSumoEnv(eval=True, csv_path=csv_path, tb_log_dir=tb_log_dir, **env_params)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()\n",
    "\n",
    "csv_path = os.path.join(csv_dir, \"untrained.csv\")\n",
    "tb_log_dir = os.path.join(\"logs\", ENV_NAME, f\"PPO_{TEST_NUM}\", \"eval_untrained\")\n",
    "\n",
    "with open(csv_path, \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow([\"sim_time\", \"arrived_num\", \"sys_tyre_pm\", \"sys_stopped\",\n",
    "                         \"sys_total_wait\", \"sys_avg_wait\", \"sys_avg_speed\",\n",
    "                         \"agents_tyre_pm\", \"agents_stopped\", \"agents_total_wait\",\n",
    "                         \"agents_avg_speed\", \"agents_total_pressure\"])\n",
    "\n",
    "register_env(ENV_NAME, lambda config: MultiAgentEnvCompatibility(eval_env_creator(csv_path, tb_log_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()\n",
    "\n",
    "register_env(ENV_NAME, lambda config: MultiAgentEnvCompatibility(train_env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TRAIN_EPS = 1400  # 720 * 1400 == 1_008_000 total timesteps\n",
    "CHECKPOINT_FREQ = 100\n",
    "assert TRAIN_EPS % CHECKPOINT_FREQ == 0\n",
    "\n",
    "tic = datetime.now()\n",
    "\n",
    "for i in range(TRAIN_EPS):\n",
    "    results = algo.train()\n",
    "\n",
    "    if (i+1) % CHECKPOINT_FREQ == 0:\n",
    "        algo.save(os.path.join(\"ray_checkpoints\",ENV_NAME,f\"test_{TEST_NUM}\"))\n",
    "\n",
    "toc = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(csv_dir, \"trained.csv\")\n",
    "tb_log_dir = os.path.join(\"logs\", ENV_NAME, f\"PPO_{TEST_NUM}\", \"eval_trained\")\n",
    "\n",
    "with open(csv_path, \"a\", newline=\"\") as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow([\"sim_time\", \"arrived_num\", \"sys_tyre_pm\", \"sys_stopped\",\n",
    "                         \"sys_total_wait\", \"sys_avg_wait\", \"sys_avg_speed\",\n",
    "                         \"agents_tyre_pm\", \"agents_stopped\", \"agents_total_wait\",\n",
    "                         \"agents_avg_speed\", \"agents_total_pressure\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env(ENV_NAME, lambda config: MultiAgentEnvCompatibility(eval_env_creator(csv_path, tb_log_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPO\n",
    "\n",
    "checkpoint_path = os.path.join(\"ray_checkpoints\",ENV_NAME,f\"test_{TEST_NUM}\",f\"checkpoint_{TRAIN_EPS:06}\")\n",
    "checkpoint_path = os.path.abspath(checkpoint_path)\n",
    "print(checkpoint_path)\n",
    "\n",
    "ppo_agent = PPO.from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
