{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid4x4 - PettingZoo + RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv  # RLlib-PZ interface\n",
    "from ray.tune.registry import register_env\n",
    "import supersuit as ss\n",
    "\n",
    "from helper_functions import make_parallel_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ed06143970>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 23423  # default SUMO seed no.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from observation import Grid4x4ObservationFunction\n",
    "from reward_functions import combined_reward\n",
    "\n",
    "def env_creator(args):\n",
    "    env_params = {\n",
    "        \"net_file\": os.path.join(\"nets\",\"grid4x4\",\"grid4x4.net.xml\"),\n",
    "        \"route_file\": os.path.join(\"nets\",\"grid4x4\",\"grid4x4_1.rou.xml\"),\n",
    "        \"num_seconds\": 3600,\n",
    "        \"reward_fn\": combined_reward,\n",
    "        \"sumo_seed\": SEED,\n",
    "        \"observation_class\": Grid4x4ObservationFunction\n",
    "    }\n",
    "    env = make_parallel_env(**env_params)\n",
    "    env = ss.pad_observations_v0(env)\n",
    "    env = ss.frame_stack_v1(env, 3)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 12:43:33,965\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "\n",
    "env_name = \"grid4x4\"\n",
    "\n",
    "register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# From https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/atari-ppo.yaml\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=env_name)\n",
    "    .framework(framework=\"torch\")\n",
    "    .rollouts(\n",
    "        rollout_fragment_length=100,\n",
    "        num_rollout_workers=10,\n",
    "        num_envs_per_worker=5,\n",
    "        batch_mode=\"truncate_episodes\",\n",
    "    )\n",
    "    .training(\n",
    "        lambda_=0.95,\n",
    "        kl_coeff=0.5,\n",
    "        clip_param=0.1,\n",
    "        vf_clip_param=10.0,\n",
    "        entropy_coeff=0.01,\n",
    "        train_batch_size=5000,\n",
    "        sgd_minibatch_size=500,\n",
    "        num_sgd_iter=10,\n",
    "    )\n",
    "    .evaluation(evaluation_num_workers=1)\n",
    "    .debugging(log_level=\"INFO\")\n",
    "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"1\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 12:43:37,422\tWARNING algorithm_config.py:784 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 81ms, vehicles TOT 0 ACT 0 BUF 0)                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m 2023-05-28 12:43:51,193\tWARNING env.py:285 -- Your MultiAgentEnv <ParallelPettingZooEnv instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24732)\u001b[0m 2023-05-28 12:43:51,204\tINFO policy.py:1285 -- Policy (worker=3) running on CPU.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24732)\u001b[0m 2023-05-28 12:43:51,204\tINFO torch_policy_v2.py:110 -- Found 0 visible cuda devices.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24732)\u001b[0m 2023-05-28 12:43:51,217\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24732)\u001b[0m 2023-05-28 12:43:51,217\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24732)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24732)\u001b[0m         StateBufferConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24732)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24732)\u001b[0m 2023-05-28 12:43:51,217\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24732)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24732)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24732)\u001b[0m         ImmutableActionsConnector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=23232)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 62ms, vehicles TOT 0 ACT 0 BUF 0)                     \u001b[32m [repeated 32x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 12:43:57,874\tINFO worker_set.py:312 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Box(0.0, 1.0, (495,), float32), Discrete(8)), '__env__': (Box(0.0, 1.0, (495,), float32), Discrete(8))}\n",
      "2023-05-28 12:43:57,911\tINFO policy.py:1285 -- Policy (worker=local) running on 1 GPUs.\n",
      "2023-05-28 12:43:57,912\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "2023-05-28 12:43:59,799\tINFO util.py:118 -- Using connectors:\n",
      "2023-05-28 12:43:59,800\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "        ObsPreprocessorConnector\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2023-05-28 12:43:59,800\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2023-05-28 12:43:59,801\tINFO rollout_worker.py:2000 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "2023-05-28 12:43:59,801\tINFO rollout_worker.py:2001 -- Built preprocessor map: {'default_policy': None}\n",
      "2023-05-28 12:43:59,802\tINFO rollout_worker.py:761 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "2023-05-28 12:43:59,818\tWARNING algorithm_config.py:784 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 53ms, vehicles TOT 0 ACT 0 BUF 0)                     \u001b[32m [repeated 18x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:08,063\tWARNING env.py:285 -- Your MultiAgentEnv <ParallelPettingZooEnv instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11512)\u001b[0m 2023-05-28 12:43:51,453\tINFO policy.py:1285 -- Policy (worker=6) running on CPU.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11512)\u001b[0m 2023-05-28 12:43:51,453\tINFO torch_policy_v2.py:110 -- Found 0 visible cuda devices.\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11512)\u001b[0m 2023-05-28 12:43:51,461\tINFO util.py:118 -- Using connectors:\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11512)\u001b[0m 2023-05-28 12:43:51,461\tINFO util.py:119 --     AgentConnectorPipeline\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11512)\u001b[0m         ObsPreprocessorConnector\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11512)\u001b[0m         StateBufferConnector\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11512)\u001b[0m         ViewRequirementAgentConnector\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11512)\u001b[0m 2023-05-28 12:43:51,461\tINFO util.py:120 --     ActionConnectorPipeline\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11512)\u001b[0m         ConvertToNumpyConnector\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11512)\u001b[0m         NormalizeActionsConnector\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11512)\u001b[0m         ImmutableActionsConnector\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 56ms, vehicles TOT 0 ACT 0 BUF 0)                     \u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 12:44:13,768\tINFO worker_set.py:312 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Box(0.0, 1.0, (495,), float32), Discrete(8)), '__env__': (Box(0.0, 1.0, (495,), float32), Discrete(8))}\n",
      "2023-05-28 12:44:13,776\tINFO policy.py:1285 -- Policy (worker=local) running on 1 GPUs.\n",
      "2023-05-28 12:44:13,777\tINFO torch_policy_v2.py:110 -- Found 1 visible cuda devices.\n",
      "2023-05-28 12:44:13,800\tINFO util.py:118 -- Using connectors:\n",
      "2023-05-28 12:44:13,800\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "        ObsPreprocessorConnector\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2023-05-28 12:44:13,801\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2023-05-28 12:44:13,802\tINFO rollout_worker.py:2000 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "2023-05-28 12:44:13,802\tINFO rollout_worker.py:2001 -- Built preprocessor map: {'default_policy': None}\n",
      "2023-05-28 12:44:13,803\tINFO rollout_worker.py:761 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "2023-05-28 12:44:13,803\tINFO trainable.py:172 -- Trainable.setup took 36.343 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 12:44:13,860\tINFO algorithm.py:935 -- Evaluating current state of PPO for 10 episodes.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m 2023-05-28 12:44:13,862\tINFO rollout_worker.py:909 -- Generating sample batch of size 5\n",
      "2023-05-28 12:47:15,348\tWARNING algorithm.py:1005 -- Calling `sample()` on your remote evaluation worker(s) resulted in a timeout (after the configured 180.0 seconds)! Try to set `evaluation_sample_timeout_s` in your config to a larger value. If your episodes don't terminate easily, you may also want to set `evaluation_duration_unit` to 'timesteps' (instead of 'episodes').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': nan,\n",
       "  'episode_reward_min': nan,\n",
       "  'episode_reward_mean': nan,\n",
       "  'episode_len_mean': nan,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 0,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [], 'episode_lengths': []},\n",
       "  'sampler_perf': {},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {},\n",
       "  'num_agent_steps_sampled_this_iter': 0,\n",
       "  'num_env_steps_sampled_this_iter': 0,\n",
       "  'timesteps_this_iter': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changes made in ParallelPettingZooEnv's reset() method: see line 202-206 in\n",
    "# C:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\n",
    "\n",
    "algo.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m 2023-05-28 12:48:15,818\tINFO rollout_worker.py:909 -- Generating sample batch of size 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=25156)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 264930ms, vehicles TOT 0 ACT 0 BUF 0)                 \u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m 2023-05-28 12:53:50,431\tWARNING env_runner_v2.py:154 -- More than 1600 observations in 100 env steps for episode 965189179399657963 are buffered in the sampler. If this is more than you expected, check that that you set a horizon on your environment correctly and that it terminates at some point. Note: In multi-agent environments, `rollout_fragment_length` sets the batch size based on (across-agents) environment steps, not the steps of individual agents, which can result in unexpectedly large batches.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m 2023-05-28 12:53:54,532\tINFO rollout_worker.py:950 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m { 'count': 500,\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((8000, 8), dtype=float32, min=-0.008, max=0.008, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'action_logp': np.ndarray((8000,), dtype=float32, min=-2.086, max=-2.073, mean=-2.079),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'actions': np.ndarray((8000,), dtype=int64, min=0.0, max=7.0, mean=3.455),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'advantages': np.ndarray((8000,), dtype=float32, min=-276.407, max=283.89, mean=-90.413),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'agent_index': np.ndarray((8000,), dtype=int32, min=0.0, max=15.0, mean=7.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'eps_id': np.ndarray((8000,), dtype=int64, min=1.1279698646374805e+17, max=9.65189179399658e+17, mean=3.501811315428677e+17),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'infos': np.ndarray((8000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'new_obs': np.ndarray((8000, 495), dtype=float32, min=0.0, max=1.0, mean=0.04),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'obs': np.ndarray((8000, 495), dtype=float32, min=0.0, max=1.0, mean=0.04),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'rewards': np.ndarray((8000,), dtype=float32, min=-62.146, max=260.251, mean=-5.889),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           't': np.ndarray((8000,), dtype=int32, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'terminateds': np.ndarray((8000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'truncateds': np.ndarray((8000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'unroll_id': np.ndarray((8000,), dtype=int32, min=16.0, max=159.0, mean=87.5),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'value_targets': np.ndarray((8000,), dtype=float32, min=-276.407, max=283.891, mean=-90.412),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m                                           'vf_preds': np.ndarray((8000,), dtype=float32, min=-0.004, max=0.006, mean=0.001)}},\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22460)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 80000\n",
      "connector_metrics: {}\n",
      "counters:\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_env_steps_sampled: 5000\n",
      "  num_env_steps_trained: 5000\n",
      "custom_metrics: {}\n",
      "date: 2023-05-28_12-54-09\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "hostname: JM-M16\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 799.5\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.5\n",
      "        cur_lr: 5.000000000000001e-05\n",
      "        entropy: 2.0785656407475472\n",
      "        entropy_coeff: 0.009999999999999998\n",
      "        kl: 0.000886725546912217\n",
      "        policy_loss: -0.002734486180188469\n",
      "        total_loss: 9.884481087327003\n",
      "        vf_explained_var: -0.0008695485442876816\n",
      "        vf_loss: 9.907557814121246\n",
      "      model: {}\n",
      "      num_grad_updates_lifetime: 800.5\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_env_steps_sampled: 5000\n",
      "  num_env_steps_trained: 5000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 80000\n",
      "num_agent_steps_trained: 80000\n",
      "num_env_steps_sampled: 5000\n",
      "num_env_steps_sampled_this_iter: 5000\n",
      "num_env_steps_trained: 5000\n",
      "num_env_steps_trained_this_iter: 5000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 10\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 5000\n",
      "perf:\n",
      "  cpu_util_percent: 3.4500650195058524\n",
      "  gpu_util_percent0: 0.3115604681404421\n",
      "  ram_util_percent: 63.557217165149545\n",
      "  vram_util_percent0: 0.07035099421427991\n",
      "pid: 21052\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "sampler_results:\n",
      "  connector_metrics: {}\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  hist_stats:\n",
      "    episode_lengths: []\n",
      "    episode_reward: []\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "time_since_restore: 353.49502873420715\n",
      "time_this_iter_s: 353.49502873420715\n",
      "time_total_s: 353.49502873420715\n",
      "timers:\n",
      "  learn_throughput: 365.267\n",
      "  learn_time_ms: 13688.6\n",
      "  load_throughput: 17008.684\n",
      "  load_time_ms: 293.967\n",
      "  sample_time_ms: 339472.032\n",
      "  synch_weights_time_ms: 23.875\n",
      "  training_iteration_time_ms: 353490.03\n",
      "timestamp: 1685274849\n",
      "timesteps_total: 5000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "result = algo.train()\n",
    "print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 12:56:53,654\tINFO algorithm.py:935 -- Evaluating current state of PPO for 10 episodes.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=14240)\u001b[0m Warning: Vehicle '478' performs emergency braking on lane ':D1_36_0' with decel=9.00, wished=4.50, severity=1.00, time=2092.00.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #2000.00 (2ms ~= 500.00*RT, ~106500.00UPS, TraCI: 1646ms, vehicles TOT 907 ACT 213 BU1212ms, vehicles TOT 1 ACT 1 BUF 0)    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 12:59:55,294\tWARNING algorithm.py:1005 -- Calling `sample()` on your remote evaluation worker(s) resulted in a timeout (after the configured 180.0 seconds)! Try to set `evaluation_sample_timeout_s` in your config to a larger value. If your episodes don't terminate easily, you may also want to set `evaluation_duration_unit` to 'timesteps' (instead of 'episodes').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': nan,\n",
       "  'episode_reward_min': nan,\n",
       "  'episode_reward_mean': nan,\n",
       "  'episode_len_mean': nan,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 0,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [], 'episode_lengths': []},\n",
       "  'sampler_perf': {},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {},\n",
       "  'num_agent_steps_sampled_this_iter': 0,\n",
       "  'num_env_steps_sampled_this_iter': 0,\n",
       "  'timesteps_this_iter': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
