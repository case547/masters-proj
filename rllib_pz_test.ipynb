{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid4x4 - PettingZoo + RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv  # RLlib-PZ interface\n",
    "from ray.tune.registry import register_env\n",
    "import supersuit as ss\n",
    "\n",
    "from helper_functions import make_parallel_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1aac302ef70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED = 23423  # default SUMO seed no.\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from observation import Grid4x4ObservationFunction\n",
    "from reward_functions import combined_reward\n",
    "\n",
    "def env_creator(args):\n",
    "    env_params = {\n",
    "        \"net_file\": os.path.join(\"nets\",\"grid4x4\",\"grid4x4.net.xml\"),\n",
    "        \"route_file\": os.path.join(\"nets\",\"grid4x4\",\"grid4x4_1.rou.xml\"),\n",
    "        \"num_seconds\": 3600,\n",
    "        \"reward_fn\": combined_reward,\n",
    "        \"sumo_seed\": SEED,\n",
    "        \"observation_class\": Grid4x4ObservationFunction\n",
    "    }\n",
    "    env = make_parallel_env(**env_params)\n",
    "    env = ss.pad_observations_v0(env)\n",
    "    env = ss.frame_stack_v1(env, 3)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 00:08:05,222\tINFO worker.py:1625 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "\n",
    "env_name = \"grid4x4\"\n",
    "\n",
    "register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# From https://github.com/ray-project/ray/blob/master/rllib/tuned_examples/ppo/atari-ppo.yaml\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(env=env_name)\n",
    "    .framework(framework=\"torch\")\n",
    "    .rollouts(\n",
    "        rollout_fragment_length=100,\n",
    "        num_rollout_workers=10,\n",
    "        num_envs_per_worker=5,\n",
    "        batch_mode=\"truncate_episodes\",\n",
    "    )\n",
    "    .training(\n",
    "        lambda_=0.95,\n",
    "        kl_coeff=0.5,\n",
    "        clip_param=0.1,\n",
    "        vf_clip_param=10.0,\n",
    "        entropy_coeff=0.01,\n",
    "        train_batch_size=5000,\n",
    "        sgd_minibatch_size=500,\n",
    "        num_sgd_iter=10,\n",
    "    )\n",
    "    .debugging(log_level=\"INFO\")\n",
    "    .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"1\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m Step #0.00 (0ms ?*RT. ?UPS, TraCI: 57ms, vehicles TOT 0 ACT 0 BUF 0)                     \u001b[32m [repeated 10x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34260)\u001b[0m Step #0.00\u001b[32m [repeated 9x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m 2023-05-26 23:53:59,268\tERROR worker.py:844 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=25312, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000259092579A0>)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m   File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m     return method(__ray_actor, *args, **kwargs)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m   File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m   File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m   File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18708\\4209285470.py\", line 5, in <lambda>\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m     assert all(\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=25312)\u001b[0m AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34260)\u001b[0m Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=34260)\u001b[0m Quitting (on error).\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "2023-05-26 23:53:59,604\tERROR actor_manager.py:507 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=26996, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002208BA4B9D0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18708\\4209285470.py\", line 5, in <lambda>\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "2023-05-26 23:53:59,605\tERROR actor_manager.py:507 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=14788, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001F08925BA30>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18708\\4209285470.py\", line 5, in <lambda>\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "2023-05-26 23:53:59,606\tERROR actor_manager.py:507 -- Ray error, taking actor 3 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=33388, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002428925BA30>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18708\\4209285470.py\", line 5, in <lambda>\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "2023-05-26 23:53:59,608\tERROR actor_manager.py:507 -- Ray error, taking actor 4 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=28160, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002100925B9D0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18708\\4209285470.py\", line 5, in <lambda>\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "2023-05-26 23:53:59,609\tERROR actor_manager.py:507 -- Ray error, taking actor 5 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=28684, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001C889263A30>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18708\\4209285470.py\", line 5, in <lambda>\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "2023-05-26 23:53:59,610\tERROR actor_manager.py:507 -- Ray error, taking actor 6 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=28080, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000022B0828FA00>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18708\\4209285470.py\", line 5, in <lambda>\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "2023-05-26 23:53:59,612\tERROR actor_manager.py:507 -- Ray error, taking actor 7 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=31100, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001818925BA00>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18708\\4209285470.py\", line 5, in <lambda>\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "2023-05-26 23:53:59,613\tERROR actor_manager.py:507 -- Ray error, taking actor 8 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=13404, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001518925B9A0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18708\\4209285470.py\", line 5, in <lambda>\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "2023-05-26 23:53:59,614\tERROR actor_manager.py:507 -- Ray error, taking actor 9 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=25312, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x00000259092579A0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18708\\4209285470.py\", line 5, in <lambda>\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n",
      "2023-05-26 23:53:59,615\tERROR actor_manager.py:507 -- Ray error, taking actor 10 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=10528, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000002470925B9A0>)\n",
      "  File \"python\\ray\\_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "  File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18708\\4209285470.py\", line 5, in <lambda>\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 184, in __init__\n",
      "    assert all(\n",
      "AssertionError: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m algo \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39;49mbuild()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm_config.py:1071\u001b[0m, in \u001b[0;36mAlgorithmConfig.build\u001b[1;34m(self, env, logger_creator, use_copy)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1069\u001b[0m     algo_class \u001b[39m=\u001b[39m get_trainable_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgo_class)\n\u001b[1;32m-> 1071\u001b[0m \u001b[39mreturn\u001b[39;00m algo_class(\n\u001b[0;32m   1072\u001b[0m     config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m use_copy \u001b[39melse\u001b[39;49;00m copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m),\n\u001b[0;32m   1073\u001b[0m     logger_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger_creator,\n\u001b[0;32m   1074\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:466\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[1;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[0;32m    459\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m    460\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mnan,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m     }\n\u001b[0;32m    464\u001b[0m }\n\u001b[1;32m--> 466\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    467\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m    468\u001b[0m     logger_creator\u001b[39m=\u001b[39mlogger_creator,\n\u001b[0;32m    469\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    470\u001b[0m )\n\u001b[0;32m    472\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:169\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[1;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout, sync_config)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mget_node_ip_address()\n\u001b[1;32m--> 169\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[0;32m    170\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:592\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[39mif\u001b[39;00m _init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    587\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m    588\u001b[0m     \u001b[39m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[0;32m    589\u001b[0m     \u001b[39m#   in each training iteration.\u001b[39;00m\n\u001b[0;32m    590\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[0;32m    591\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[0;32m    593\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[0;32m    594\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[0;32m    595\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[0;32m    596\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[0;32m    597\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rollout_workers,\n\u001b[0;32m    598\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    599\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[0;32m    600\u001b[0m     )\n\u001b[0;32m    602\u001b[0m     \u001b[39m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[0;32m    603\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[0;32m    604\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[0;32m    605\u001b[0m         \u001b[39m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:194\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[1;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    182\u001b[0m     \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[0;32m    183\u001b[0m     \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[39m# errors.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mactor_init_failed:\n\u001b[0;32m    187\u001b[0m         \u001b[39m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[0;32m    188\u001b[0m         \u001b[39m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[39m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[0;32m    193\u001b[0m         \u001b[39m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m         \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m2\u001b[39m]\n\u001b[0;32m    195\u001b[0m     \u001b[39m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:870\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:921\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:877\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:881\u001b[0m, in \u001b[0;36mray._raylet.execute_task\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:821\u001b[0m, in \u001b[0;36mray._raylet.execute_task.function_executor\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py:670\u001b[0m, in \u001b[0;36mactor_method_executor\u001b[1;34m()\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 670\u001b[0m     \u001b[39mreturn\u001b[39;00m method(__ray_actor, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py:460\u001b[0m, in \u001b[0;36m_resume_span\u001b[1;34m()\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39m# If tracing feature flag is not on, perform a no-op\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_tracing_enabled() \u001b[39mor\u001b[39;00m _ray_trace_ctx \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[0;32m    462\u001b[0m tracer: _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mTracer \u001b[39m=\u001b[39m _opentelemetry\u001b[39m.\u001b[39mtrace\u001b[39m.\u001b[39mget_tracer(\n\u001b[0;32m    463\u001b[0m     \u001b[39m__name__\u001b[39m\n\u001b[0;32m    464\u001b[0m )\n\u001b[0;32m    466\u001b[0m \u001b[39m# Retrieves the context from the _ray_trace_ctx dictionary we\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39m# injected.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\evaluation\\rollout_worker.py:609\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m()\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[39m# Create a (single) env for this worker.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[0;32m    604\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworker_index \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    605\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_workers \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    606\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mcreate_env_on_local_worker\n\u001b[0;32m    607\u001b[0m ):\n\u001b[0;32m    608\u001b[0m     \u001b[39m# Run the `env_creator` function passing the EnvContext.\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39m=\u001b[39m env_creator(copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_context))\n\u001b[0;32m    611\u001b[0m clip_rewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mclip_rewards\n\u001b[0;32m    613\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    614\u001b[0m     \u001b[39m# Validate environment (general validation function).\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m ray\u001b[39m.\u001b[39minit()\n\u001b[0;32m      3\u001b[0m env_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgrid4x4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m register_env(env_name, \u001b[39mlambda\u001b[39;00m config: ParallelPettingZooEnv(env_creator(config)))\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py:184\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m()\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39m# Get first action space, assuming all agents have equal space\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpar_env\u001b[39m.\u001b[39maction_space(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpar_env\u001b[39m.\u001b[39magents[\u001b[39m0\u001b[39m])\n\u001b[1;32m--> 184\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpar_env\u001b[39m.\u001b[39mobservation_space(agent) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space\n\u001b[0;32m    186\u001b[0m     \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpar_env\u001b[39m.\u001b[39magents\n\u001b[0;32m    187\u001b[0m ), (\n\u001b[0;32m    188\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mObservation spaces for all agents must be identical. Perhaps \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    189\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mSuperSuit\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms pad_observations wrapper can help (useage: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`supersuit.aec_wrappers.pad_observations(env)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m )\n\u001b[0;32m    193\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[0;32m    194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpar_env\u001b[39m.\u001b[39maction_space(agent) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\n\u001b[0;32m    195\u001b[0m     \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpar_env\u001b[39m.\u001b[39magents\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m`supersuit.aec_wrappers.pad_action_space(env)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Observation spaces for all agents must be identical. Perhaps SuperSuit's pad_observations wrapper can help (useage: `supersuit.aec_wrappers.pad_observations(env)`"
     ]
    }
   ],
   "source": [
    "algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 12:08:00,843\tINFO rollout_worker.py:909 -- Generating sample batch of size 4000\n",
      "2023-05-02 12:09:19,946\tWARNING env_runner_v2.py:154 -- More than 11520 observations in 720 env steps for episode 391696027291684039 are buffered in the sampler. If this is more than you expected, check that that you set a horizon on your environment correctly and that it terminates at some point. Note: In multi-agent environments, `rollout_fragment_length` sets the batch size based on (across-agents) environment steps, not the steps of individual agents, which can result in unexpectedly large batches.Also, you may be waiting for your Env to terminate (batch_mode=`complete_episodes`). Make sure it does at some point.\n",
      "2023-05-02 12:11:57,251\tWARNING env_runner_v2.py:154 -- More than 11520 observations in 720 env steps for episode 686425610722139992 are buffered in the sampler. If this is more than you expected, check that that you set a horizon on your environment correctly and that it terminates at some point. Note: In multi-agent environments, `rollout_fragment_length` sets the batch size based on (across-agents) environment steps, not the steps of individual agents, which can result in unexpectedly large batches.Also, you may be waiting for your Env to terminate (batch_mode=`complete_episodes`). Make sure it does at some point.\n",
      "Exception ignored in: <function SumoEnvironment.__del__ at 0x000001BA2C127250>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\sumo_rl\\environment\\env.py\", line 446, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\sumo_rl\\environment\\env.py\", line 436, in close\n",
      "    traci.close()\n",
      "  File \"C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\main.py\", line 263, in close\n",
      "    _connections[\"\"].close(wait)\n",
      "  File \"C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\", line 355, in close\n",
      "    self._sendCmd(tc.CMD_CLOSE, None, None)\n",
      "  File \"C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\", line 189, in _sendCmd\n",
      "    return self._sendExact()\n",
      "  File \"C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\", line 106, in _sendExact\n",
      "    raise TraCIException(err, prefix[1], _RESULTS[prefix[2]])\n",
      "KeyError: 21\n",
      "2023-05-02 12:14:11,913\tWARNING env_runner_v2.py:154 -- More than 11520 observations in 720 env steps for episode 270665924183041501 are buffered in the sampler. If this is more than you expected, check that that you set a horizon on your environment correctly and that it terminates at some point. Note: In multi-agent environments, `rollout_fragment_length` sets the batch size based on (across-agents) environment steps, not the steps of individual agents, which can result in unexpectedly large batches.Also, you may be waiting for your Env to terminate (batch_mode=`complete_episodes`). Make sure it does at some point.\n",
      "2023-05-02 12:15:18,077\tINFO rollout_worker.py:950 -- Completed sample batch:\n",
      "\n",
      "{ 'count': 4320,\n",
      "  'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((69120, 8), dtype=float32, min=-0.005, max=0.007, mean=0.0),\n",
      "                                          'action_logp': np.ndarray((69120,), dtype=float32, min=-2.084, max=-2.074, mean=-2.079),\n",
      "                                          'actions': np.ndarray((69120,), dtype=int64, min=0.0, max=7.0, mean=3.492),\n",
      "                                          'advantages': np.ndarray((69120,), dtype=float32, min=-535.331, max=2019.548, mean=-0.545),\n",
      "                                          'agent_index': np.ndarray((69120,), dtype=int32, min=0.0, max=15.0, mean=7.5),\n",
      "                                          'eps_id': np.ndarray((69120,), dtype=int64, min=2.706659241830415e+17, max=9.611581254342465e+17, mean=6.514687525304192e+17),\n",
      "                                          'infos': np.ndarray((69120,), dtype=object, head={}),\n",
      "                                          'new_obs': np.ndarray((69120, 33), dtype=float32, min=0.0, max=1.0, mean=0.052),\n",
      "                                          'obs': np.ndarray((69120, 33), dtype=float32, min=0.0, max=1.0, mean=0.052),\n",
      "                                          'rewards': np.ndarray((69120,), dtype=float32, min=-125.0, max=852.0, mean=-0.042),\n",
      "                                          't': np.ndarray((69120,), dtype=int32, min=0.0, max=719.0, mean=359.5),\n",
      "                                          'terminateds': np.ndarray((69120,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "                                          'truncateds': np.ndarray((69120,), dtype=bool, min=0.0, max=1.0, mean=0.001),\n",
      "                                          'unroll_id': np.ndarray((69120,), dtype=int32, min=400.0, max=575.0, mean=487.5),\n",
      "                                          'value_targets': np.ndarray((69120,), dtype=float32, min=-535.332, max=2019.552, mean=-0.543),\n",
      "                                          'vf_preds': np.ndarray((69120,), dtype=float32, min=-0.005, max=0.006, mean=0.001)}},\n",
      "  'type': 'MultiAgentBatch'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 69120\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.06513198216756184\n",
      "  StateBufferConnector_ms: 0.0\n",
      "  ViewRequirementAgentConnector_ms: 0.6788134574890137\n",
      "counters:\n",
      "  num_agent_steps_sampled: 69120\n",
      "  num_agent_steps_trained: 69120\n",
      "  num_env_steps_sampled: 4320\n",
      "  num_env_steps_trained: 4320\n",
      "custom_metrics: {}\n",
      "date: 2023-05-02_12-15-25\n",
      "done: false\n",
      "episode_len_mean: 720.0\n",
      "episode_media: {}\n",
      "episode_reward_max: -246.0\n",
      "episode_reward_mean: -482.1666666666667\n",
      "episode_reward_min: -732.0\n",
      "episodes_this_iter: 6\n",
      "episodes_total: 6\n",
      "hostname: JM-M16\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 539.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.19999999999999998\n",
      "        cur_lr: 0.00025\n",
      "        entropy: 2.0776924362889044\n",
      "        entropy_coeff: 0.001\n",
      "        grad_gnorm: 0.426115154116242\n",
      "        kl: 0.0017535451632545152\n",
      "        policy_loss: -0.002954182497988869\n",
      "        total_loss: 9.377007239394718\n",
      "        vf_explained_var: -0.002420284461092066\n",
      "        vf_loss: 9.381688413355086\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 256.0\n",
      "      num_grad_updates_lifetime: 540.5\n",
      "  num_agent_steps_sampled: 69120\n",
      "  num_agent_steps_trained: 69120\n",
      "  num_env_steps_sampled: 4320\n",
      "  num_env_steps_trained: 4320\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 69120\n",
      "num_agent_steps_trained: 69120\n",
      "num_env_steps_sampled: 4320\n",
      "num_env_steps_sampled_this_iter: 4320\n",
      "num_env_steps_trained: 4320\n",
      "num_env_steps_trained_this_iter: 4320\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 0\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4320\n",
      "perf:\n",
      "  cpu_util_percent: 1.1796324655436445\n",
      "  ram_util_percent: 42.41699846860643\n",
      "pid: 19532\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.6885713406443402\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 95.40613589808775\n",
      "  mean_inference_ms: 1.968029333854875\n",
      "  mean_raw_obs_processing_ms: 3.079875424293919\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.06513198216756184\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.6788134574890137\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 720.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -246.0\n",
      "  episode_reward_mean: -482.1666666666667\n",
      "  episode_reward_min: -732.0\n",
      "  episodes_this_iter: 6\n",
      "  hist_stats:\n",
      "    episode_lengths: [720, 720, 720, 720, 720, 720]\n",
      "    episode_reward: [-347.0, -314.0, -604.0, -650.0, -246.0, -732.0]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.6885713406443402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 95.40613589808775\n",
      "    mean_inference_ms: 1.968029333854875\n",
      "    mean_raw_obs_processing_ms: 3.079875424293919\n",
      "time_since_restore: 444.96659445762634\n",
      "time_this_iter_s: 444.96659445762634\n",
      "time_total_s: 444.96659445762634\n",
      "timers:\n",
      "  learn_throughput: 558.786\n",
      "  learn_time_ms: 7731.047\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  sample_time_ms: 437234.548\n",
      "  synch_weights_time_ms: 0.0\n",
      "  training_iteration_time_ms: 444965.595\n",
      "timestamp: 1683026125\n",
      "timesteps_total: 4320\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "result = algo.train()\n",
    "print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=12976)\u001b[0m 2023-05-02 00:20:14,573\tWARNING algorithm_config.py:635 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 881, in ray._raylet.execute_task\u001b[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m   File \"python\\ray\\_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m   File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 670, in actor_method_executor\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m     return method(__ray_actor, *args, **kwargs)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m   File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 460, in _resume_span\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m   File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\rllib\\env\\wrappers\\pettingzoo_env.py\", line 169, in __init__\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m   File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21328\\2883675884.py\", line 5, in <lambda>\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m     self.par_env.reset()\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m   File \"c:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\pettingzoo\\utils\\conversions.py\", line 126, in reset\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m     observations = super().reset(seed=seed, options=options)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m     res = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m     self.aec_env.reset(seed=seed, return_info=return_info, options=options)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m TypeError: BaseWrapper.reset() got an unexpected keyword argument 'return_info'\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11780)\u001b[0m 2023-05-02 00:20:00,542\tERROR worker.py:844 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=11780, ip=127.0.0.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000022A8925B820>)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_pistonball_v6_b5f2c_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tune\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m      2\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPPO\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      3\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPPO\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      4\u001b[0m     stop\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mtimesteps_total\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m500\u001b[39;49m},\n\u001b[0;32m      5\u001b[0m     checkpoint_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      6\u001b[0m     local_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m~/ray_results/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m env_name,\n\u001b[0;32m      7\u001b[0m     config\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mto_dict(),\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\marl\\lib\\site-packages\\ray\\tune\\tune.py:939\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, trial_executor, _experiment_checkpoint_dir, _remote, _remote_string_queue, _tuner_api)\u001b[0m\n\u001b[0;32m    937\u001b[0m \u001b[39mif\u001b[39;00m incomplete_trials:\n\u001b[0;32m    938\u001b[0m     \u001b[39mif\u001b[39;00m raise_on_failed_trial \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m experiment_interrupted_event\u001b[39m.\u001b[39mis_set():\n\u001b[1;32m--> 939\u001b[0m         \u001b[39mraise\u001b[39;00m TuneError(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete\u001b[39m\u001b[39m\"\u001b[39m, incomplete_trials)\n\u001b[0;32m    940\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m         logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mTrials did not complete: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, incomplete_trials)\n",
      "\u001b[1;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_pistonball_v6_b5f2c_00000])"
     ]
    }
   ],
   "source": [
    "# tune.run(\n",
    "#     \"PPO\",\n",
    "#     name=\"PPO\",\n",
    "#     stop={\"timesteps_total\": 500},\n",
    "#     checkpoint_freq=10,\n",
    "#     local_dir=\"~/ray_results/\" + env_name,\n",
    "#     config=config.to_dict(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
